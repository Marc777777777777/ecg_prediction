{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bM9r6fS2QO7"
      },
      "source": [
        "# **Project Machine Learning: Using methods of machine learning with ECG to classify cardiac diseases**\n",
        "\n",
        "**Group:**\\\n",
        "Rithy SOCHET\\\n",
        "Marc KASPAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9o4MkQ--_K2"
      },
      "outputs": [],
      "source": [
        "# Import :\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns   # For data visualisation\n",
        "\n",
        "# Import of the different models :\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Import of the preprocessing functions:\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Import the Pipeline and Cross validation functions:\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data :\n",
        "data = pd.read_csv(\"ECGCvdata.csv\")"
      ],
      "metadata": {
        "id": "wDhrVlk6_xVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6hHWQvBTmt6"
      },
      "source": [
        "Link of the dataset : https://www.kaggle.com/datasets/akki2703/ecg-of-cardiac-ailments-dataset\n",
        "\n",
        "The file consists of 1200 records of cardiac ailments.\n",
        "\n",
        "Each 300 records belongs to one class of ailment and 56 columns :\n",
        "\n",
        "Column 1 give number of records.\n",
        "\n",
        "Column 56 give the class of the record :\n",
        "- Class 0 : NSR - Normal Sinus Rhythm (Normal situation)\n",
        "- Class 1 : ARR - Arrhythmia\n",
        "- Class 2 : AFF - Atrial Fibrillation\n",
        "- Class 3 : CHF - Congestive heart failure\n",
        "\n",
        "So we deal with a Multiclass classification problem.\n",
        "\n",
        "The 54 remaining columns are features extracted using MODWPT method. The original signals are taken from MIT-BIH physionet database and are processed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqR7LGw8EB2X"
      },
      "source": [
        "# Exploration of the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmT4B_zoEGVH"
      },
      "outputs": [],
      "source": [
        "# Printing the first few lines:\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb0lWUKzEGiP"
      },
      "outputs": [],
      "source": [
        "# Printing a summary of the data:\n",
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaUnrA9NF-tN"
      },
      "outputs": [],
      "source": [
        "# Let's look for the number of missing values:\n",
        "missing_values = data.isnull().sum()\n",
        "missing_values = missing_values[missing_values > 0]\n",
        "print(missing_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-k5v3MV8o9a"
      },
      "outputs": [],
      "source": [
        "# Let's see the distribution of the 4 classes in the dataset:\n",
        "class_count = data.groupby(\"ECG_signal\").size()\n",
        "classes = class_count.index.values\n",
        "count = class_count.values\n",
        "plt.bar(classes, count, color=[\"blue\", \"orange\", \"green\", \"red\"])\n",
        "plt.title(\"Number of records per classes\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxTxjshxENNO"
      },
      "outputs": [],
      "source": [
        "# Correlation matrix and heatmap:\n",
        "correlation_matrix = data.corr(numeric_only=True)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "sns.heatmap(correlation_matrix, square=True, linecolor=\"White\", linewidths=0.1, center=0)\n",
        "plt.title(\"Heatmap of the dataset\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvpsLm7X5nmQ"
      },
      "outputs": [],
      "source": [
        "# We will select some features to visualise:\n",
        "features = [\"hbpermin\", \"QRSarea\", \"QRSperi\", \"RRmean\"]\n",
        "\n",
        "# We plot these features pairwise:\n",
        "sns.pairplot(data, vars=features, hue=\"ECG_signal\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Another way to visualize data: Boxplots\n",
        "for f in features:\n",
        "    sns.boxplot(data=data, x=\"ECG_signal\", y=f, hue=\"ECG_signal\")\n",
        "    plt.title(\"Distribution of \"+ f)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NovuGi8HML_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPq00F_E_Z66"
      },
      "outputs": [],
      "source": [
        "# Let's now prepare the data for the training of our models :\n",
        "data = data.to_numpy()\n",
        "X_data = data[:, 1:-1]   # The first column is the ID of the record so we do not consider it.\n",
        "y_data = data[:,-1]      # The last column is the class.\n",
        "\n",
        "# Change the values of y by 0, 1, 2 or 3 depending on the class :\n",
        "y_data[y_data == \"NSR\"] = 0\n",
        "y_data[y_data == \"ARR\"] = 1\n",
        "y_data[y_data == \"AFF\"] = 2\n",
        "y_data[y_data == \"CHF\"] = 3\n",
        "y_data = np.array(y_data, dtype=int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LFC233jPbDe"
      },
      "source": [
        "# I. Let's test some models using only 2 classes (AFF and CHF) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3Wn6sNVPm4a"
      },
      "outputs": [],
      "source": [
        "# For this part we will consider the following 2 classes :\n",
        "# class 2 : AFF - Atrial Fibrillation\n",
        "# class 3 : CHF - Congestive heart failure\n",
        "y_class2 = y_data[y_data == 2]\n",
        "y_class3 = y_data[y_data == 3]\n",
        "y_1 = np.concatenate((y_class2, y_class3))\n",
        "\n",
        "X_class0 = X_data[y_data==2 , :]\n",
        "X_class1 = X_data[y_data==3, :]\n",
        "X_1 = np.vstack((X_class0, X_class1))\n",
        "\n",
        "# We split the dataset into a train set and a test set:\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(\n",
        "    X_1, y_1, stratify = y_1, test_size = 0.2, random_state = 42)\n",
        "\n",
        "# We store the classifier used and their accuracy on the test set:\n",
        "classifier_list = []\n",
        "accuracy_list = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nH_JlJFzPnOi"
      },
      "outputs": [],
      "source": [
        "# Pipeline of LDA :\n",
        "\n",
        "pipeline_LDA = Pipeline(\n",
        "    [\n",
        "        ( \"imputer\", SimpleImputer() ),                     # Imputer\n",
        "        ( \"scaler\", StandardScaler() ),                     # Standardisation\n",
        "        ( \"classifier_LDA\", LinearDiscriminantAnalysis(solver=\"lsqr\") )  # Classifier LDA\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = [\n",
        "    {\n",
        "        \"scaler\" : [StandardScaler(), \"passthrough\"],       # Standardisation on/off,\n",
        "        \"imputer__strategy\" : [\"mean\" , \"median\"],          # Mean vs Median imputer\n",
        "        \"classifier_LDA__shrinkage\" : [None,\"auto\",0.01,0.1,0.3]    # Shrinkage parameters\n",
        "    }\n",
        "]\n",
        "\n",
        "rskf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
        "\n",
        "cv = GridSearchCV(pipeline_LDA, params, cv=rskf, scoring=\"accuracy\", n_jobs=-1)\n",
        "\n",
        "cv.fit(X_train, y_train)\n",
        "print(f'Best accuracy score: {cv.best_score_:.3f}\\n')\n",
        "print(f'Best parameter set: {cv.best_params_}\\n')\n",
        "print(\"Train Scores:\")\n",
        "print(f'{classification_report(y_train, cv.predict(X_train))}')\n",
        "\n",
        "preds = cv.predict(X_holdout)\n",
        "print(\"Test Scores:\")\n",
        "print(f'{classification_report(y_holdout, preds)}\\n')\n",
        "print(f'Test accuracy score: {accuracy_score(y_holdout, preds):.3f}')\n",
        "\n",
        "classifier_list.append(\"LDA\")\n",
        "accuracy_list.append(accuracy_score(y_holdout, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2SF9EeyR0FN"
      },
      "outputs": [],
      "source": [
        "# Pipeline of Logistic Regression :\n",
        "\n",
        "pipeline_LogReg = Pipeline(\n",
        "    [\n",
        "        ( \"imputer\", SimpleImputer() ),                     # Imputer\n",
        "        ( \"scaler\", StandardScaler() ),                     # Standardisation\n",
        "        ( \"classifier_LogReg\", LogisticRegression() )       # Classifier Logistic Regression\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = [\n",
        "    {\n",
        "        \"scaler\" : [StandardScaler(), \"passthrough\"],       # Standardisation on/off,\n",
        "        \"imputer__strategy\" : [\"mean\" , \"median\"],          # Mean vs Median imputer\n",
        "        \"classifier_LogReg__penalty\" : [\"l2\", None]         # Penalty used for regularisation\n",
        "    }\n",
        "]\n",
        "\n",
        "rskf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
        "\n",
        "cv = GridSearchCV(pipeline_LogReg, params, cv=rskf, scoring=\"accuracy\", n_jobs=-1)\n",
        "\n",
        "cv.fit(X_train, y_train)\n",
        "print(f'Best accuracy score: {cv.best_score_:.3f}\\n')\n",
        "print(f'Best parameter set: {cv.best_params_}\\n')\n",
        "print(\"Train Scores:\")\n",
        "print(f'{classification_report(y_train, cv.predict(X_train))}')\n",
        "\n",
        "preds = cv.predict(X_holdout)\n",
        "print(\"Test Scores:\")\n",
        "print(f'{classification_report(y_holdout, preds)}\\n')\n",
        "print(f'Test accuracy score: {accuracy_score(y_holdout, preds):.3f}')\n",
        "\n",
        "classifier_list.append(\"Logistic Regression\")\n",
        "accuracy_list.append(accuracy_score(y_holdout, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMqZrojuSBnB"
      },
      "outputs": [],
      "source": [
        "# Pipeline of Decisions Trees :\n",
        "\n",
        "pipeline_DecTree = Pipeline(\n",
        "    [\n",
        "        ( \"imputer\", SimpleImputer() ),                     # Imputer\n",
        "        ( \"scaler\", StandardScaler() ),                     # Standardisation\n",
        "        ( \"classifier_DecisionTree\", DecisionTreeClassifier() )       # Classifier Decision Trees\n",
        "\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = [\n",
        "    {\n",
        "        \"scaler\" : [StandardScaler(), \"passthrough\"],        # Standardisation on/off,\n",
        "        \"imputer__strategy\" : [\"mean\" , \"median\"],           # Mean vs Median imputer\n",
        "        \"classifier_DecisionTree__criterion\" : [\"gini\", \"entropy\", \"log_loss\"],     # Spliting criterion\n",
        "        \"classifier_DecisionTree__splitter\" : [\"best\", \"random\"],                   # Strategy for splitting\n",
        "        \"classifier_DecisionTree__max_depth\" : [i for i in range(10,31,5)]          # Maximum depth\n",
        "    }\n",
        "]\n",
        "\n",
        "rskf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
        "\n",
        "cv = GridSearchCV(pipeline_DecTree, params, cv=rskf, scoring=\"accuracy\", n_jobs=-1)\n",
        "\n",
        "cv.fit(X_train, y_train)\n",
        "print(f'Best accuracy score: {cv.best_score_:.3f}\\n')\n",
        "print(f'Best parameter set: {cv.best_params_}\\n')\n",
        "print(\"Train Scores:\")\n",
        "print(f'{classification_report(y_train, cv.predict(X_train))}')\n",
        "\n",
        "preds = cv.predict(X_holdout)\n",
        "print(\"Test Scores:\")\n",
        "print(f'{classification_report(y_holdout, preds)}\\n')\n",
        "print(f'Test accuracy score: {accuracy_score(y_holdout, preds):.3f}')\n",
        "\n",
        "classifier_list.append(\"Decisions Trees\")\n",
        "accuracy_list.append(accuracy_score(y_holdout, preds))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary:\n",
        "print(\"Performance with class 2 and 3\")\n",
        "print(\"Here are the differents accuracy score obtained after getting the best parameters for each classifier:\")\n",
        "for i in range(3):\n",
        "    print(\"Classifier: \" + classifier_list[i] + \", test accuracy = \", accuracy_list[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Plila4rRwXN",
        "outputId": "951aaeed-d18f-48fb-9549-dcd888f230f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance with class 2 and 3\n",
            "Here are the differents accuracy score obtained after getting the best parameters for each classifier:\n",
            "Classifier: LDA, test accuracy =  0.9583333333333334\n",
            "Classifier: Logistic Regression, test accuracy =  0.9583333333333334\n",
            "Classifier: Decisions Trees, test accuracy =  0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soS7rHvuvzw9"
      },
      "source": [
        "# II. Model considering 2 classes : normal ECG and abnormal ECG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHaqFgDvRvPC"
      },
      "source": [
        "We will try different models and use an accuracy function to determine which model is the most appropriate for our problem.\n",
        "\n",
        "We will try LDA, Logistic Regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBKvNXWqXBiM"
      },
      "outputs": [],
      "source": [
        "# For this part we will consider the following 2 classes :\n",
        "# class 0 : NSR - Normal Sinus Rhythm (Normal situation)\n",
        "# class 1 : The other classes (Abnormal situation)\n",
        "y_class0 = y_data[y_data == 0]\n",
        "y_class1 = y_data[y_data != 0]\n",
        "y_class1[:] = 1\n",
        "y_2 = np.concatenate((y_class0, y_class1))\n",
        "\n",
        "X_class0 = X_data[y_data==0 , :]\n",
        "X_class1 = X_data[y_data!=0, :]\n",
        "X_2 = np.vstack((X_class0, X_class1))\n",
        "\n",
        "# We split the dataset into a train set and a test set:\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(\n",
        "    X_2, y_2, stratify = y_2, test_size = 0.2, random_state = 42)\n",
        "\n",
        "# We store the classifier used and their accuracy on the test set:\n",
        "classifier_list = []\n",
        "accuracy_list = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6-EYa3q_Zuu"
      },
      "outputs": [],
      "source": [
        "# Pipeline of LDA :\n",
        "\n",
        "pipeline_LDA = Pipeline(\n",
        "    [\n",
        "        ( \"imputer\", SimpleImputer() ),                     # Imputer\n",
        "        ( \"scaler\", StandardScaler() ),                     # Standardisation\n",
        "        ( \"classifier_LDA\", LinearDiscriminantAnalysis(solver=\"lsqr\") )  # Classifier LDA\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = [\n",
        "    {\n",
        "        \"scaler\" : [StandardScaler(), \"passthrough\"],       # Standardisation on/off,\n",
        "        \"imputer__strategy\" : [\"mean\" , \"median\"],          # Mean vs Median imputer\n",
        "        \"classifier_LDA__shrinkage\" : [None,\"auto\",0.01,0.1,0.3]    # Shrinkage parameter\n",
        "    }\n",
        "]\n",
        "\n",
        "rskf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
        "\n",
        "cv = GridSearchCV(pipeline_LDA, params, cv=rskf, scoring=\"accuracy\", n_jobs=-1)\n",
        "\n",
        "cv.fit(X_train, y_train)\n",
        "print(f'Best accuracy score: {cv.best_score_:.3f}\\n')\n",
        "print(f'Best parameter set: {cv.best_params_}\\n')\n",
        "print(\"Train Scores:\")\n",
        "print(f'{classification_report(y_train, cv.predict(X_train))}')\n",
        "\n",
        "preds = cv.predict(X_holdout)\n",
        "print(\"Test Scores:\")\n",
        "print(f'{classification_report(y_holdout, preds)}\\n')\n",
        "print(f'Test accuracy score: {accuracy_score(y_holdout, preds):.3f}')\n",
        "\n",
        "classifier_list.append(\"LDA\")\n",
        "accuracy_list.append(accuracy_score(y_holdout, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZvtPLYRQYm1"
      },
      "outputs": [],
      "source": [
        "# Pipeline of Logistic Regression :\n",
        "\n",
        "pipeline_LogReg = Pipeline(\n",
        "    [\n",
        "        ( \"imputer\", SimpleImputer() ),                     # Imputer\n",
        "        ( \"scaler\", StandardScaler() ),                     # Standardisation\n",
        "        ( \"classifier_LogReg\", LogisticRegression() )       # Classifier Logistic Regression\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = [\n",
        "    {\n",
        "        \"scaler\" : [StandardScaler(), \"passthrough\"],       # Standardisation on/off,\n",
        "        \"imputer__strategy\" : [\"mean\" , \"median\"],          # Mean vs Median imputer\n",
        "        \"classifier_LogReg__penalty\" : [\"l2\", None]         # Penalty for regularisation\n",
        "    }\n",
        "]\n",
        "\n",
        "rskf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
        "\n",
        "cv = GridSearchCV(pipeline_LogReg, params, cv=rskf, scoring=\"accuracy\", n_jobs=-1)\n",
        "\n",
        "cv.fit(X_train, y_train)\n",
        "print(f'Best accuracy score: {cv.best_score_:.3f}\\n')\n",
        "print(f'Best parameter set: {cv.best_params_}\\n')\n",
        "print(\"Train Scores:\")\n",
        "print(f'{classification_report(y_train, cv.predict(X_train))}')\n",
        "\n",
        "preds = cv.predict(X_holdout)\n",
        "print(\"Test Scores:\")\n",
        "print(f'{classification_report(y_holdout, preds)}\\n')\n",
        "print(f'Test accuracy score: {accuracy_score(y_holdout, preds):.3f}')\n",
        "\n",
        "classifier_list.append(\"Logistic Regression\")\n",
        "accuracy_list.append(accuracy_score(y_holdout, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EW8thcLYqvy"
      },
      "outputs": [],
      "source": [
        "# Pipeline of Decisions Trees :\n",
        "\n",
        "pipeline_DecTree = Pipeline(\n",
        "    [\n",
        "        ( \"imputer\", SimpleImputer() ),                     # Imputer\n",
        "        ( \"scaler\", StandardScaler() ),                     # Standardisation\n",
        "        ( \"classifier_DecisionTree\", DecisionTreeClassifier() )       # Classifier Decision Trees\n",
        "\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = [\n",
        "    {\n",
        "        \"scaler\" : [StandardScaler(), \"passthrough\"],       # Standardisation on/off,\n",
        "        \"imputer__strategy\" : [\"mean\" , \"median\"],          # Mean vs Median imputer\n",
        "        \"classifier_DecisionTree__criterion\" : [\"gini\", \"entropy\", \"log_loss\"],     # Splitting criterion\n",
        "        \"classifier_DecisionTree__splitter\" : [\"best\", \"random\"],                   # Strategy for splitting\n",
        "        \"classifier_DecisionTree__max_depth\" : [i for i in range(10,31,5)]          # Maximum depth\n",
        "    }\n",
        "]\n",
        "\n",
        "rskf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
        "\n",
        "cv = GridSearchCV(pipeline_DecTree, params, cv=rskf, scoring=\"accuracy\", n_jobs=-1)\n",
        "\n",
        "cv.fit(X_train, y_train)\n",
        "print(f'Best accuracy score: {cv.best_score_:.3f}\\n')\n",
        "print(f'Best parameter set: {cv.best_params_}\\n')\n",
        "print(\"Train Scores:\")\n",
        "print(f'{classification_report(y_train, cv.predict(X_train))}')\n",
        "\n",
        "preds = cv.predict(X_holdout)\n",
        "print(\"Test Scores:\")\n",
        "print(f'{classification_report(y_holdout, preds)}\\n')\n",
        "print(f'Test accuracy score: {accuracy_score(y_holdout, preds):.3f}')\n",
        "\n",
        "classifier_list.append(\"Decision Trees\")\n",
        "accuracy_list.append(accuracy_score(y_holdout, preds))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary:\n",
        "print(\"Performance with class normal and anormal\")\n",
        "print(\"Here are the differents accuracy score obtained after getting the best parameters for each classifier:\")\n",
        "for i in range(3):\n",
        "    print(\"Classifier: \" + classifier_list[i] + \", test accuracy = \", accuracy_list[i])"
      ],
      "metadata": {
        "id": "bdBsnP-KTRcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXXfoHfyv-Do"
      },
      "source": [
        "# III. Model considering the 4 classes :"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We split the dataset into a train set and a test set:\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(\n",
        "    X_data, y_data, stratify = y_data, test_size = 0.2, random_state = 42)\n",
        "\n",
        "# We store the classifier used and their accuracy on the test set:\n",
        "classifier_list = []\n",
        "accuracy_list = []\n"
      ],
      "metadata": {
        "id": "sSJPJ-z4T7FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxYYkT6tjhwX"
      },
      "outputs": [],
      "source": [
        "# Pipeline of LDA :\n",
        "\n",
        "pipeline_LDA = Pipeline(\n",
        "    [\n",
        "        ( \"imputer\", SimpleImputer() ),                          # Imputer\n",
        "        ( \"scaler\", StandardScaler() ),                          # Standardisation\n",
        "        ( \"classifier_LDA\", LinearDiscriminantAnalysis(solver=\"lsqr\") )       # Classifier LDA\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = [\n",
        "    {\n",
        "        \"scaler\" : [StandardScaler(), \"passthrough\"],        # Standardisation on/off,\n",
        "        \"imputer__strategy\" : [\"mean\" , \"median\"],           # Mean vs Median imputer\n",
        "        \"classifier_LDA__shrinkage\" : [None,\"auto\",0.01,0.1,0.3]    # Shrinkage parameter\n",
        "    }\n",
        "]\n",
        "\n",
        "rskf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
        "\n",
        "cv = GridSearchCV(pipeline_LDA, params, cv=rskf, scoring=\"accuracy\", n_jobs=-1)\n",
        "\n",
        "cv.fit(X_train, y_train)\n",
        "print(f'Best accuracy score: {cv.best_score_:.3f}\\n')\n",
        "print(f'Best parameter set: {cv.best_params_}\\n')\n",
        "print(\"Train Scores:\")\n",
        "print(f'{classification_report(y_train, cv.predict(X_train))}')\n",
        "\n",
        "preds = cv.predict(X_holdout)\n",
        "print(\"Test Scores:\")\n",
        "print(f'{classification_report(y_holdout, preds)}\\n')\n",
        "print(f'Test accuracy score: {accuracy_score(y_holdout, preds):.3f}')\n",
        "\n",
        "classifier_list.append(\"LDA\")\n",
        "accuracy_list.append(accuracy_score(y_holdout, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOYyVL8FWLEI"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(preds, y_holdout)\n",
        "\n",
        "print(\"Confusion Matrix for the LDA:\")\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQx_BBxKwBx2"
      },
      "outputs": [],
      "source": [
        "# Pipeline of Logistic Regression :\n",
        "\n",
        "pipeline_LogReg = Pipeline(\n",
        "    [\n",
        "        ( \"imputer\", SimpleImputer() ),                     # Imputer\n",
        "        ( \"scaler\", StandardScaler() ),                     # Standardisation\n",
        "        ( \"classifier_LogReg\", LogisticRegression() )       # Classifier Logistic Regression\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = [\n",
        "    {\n",
        "        \"scaler\" : [StandardScaler(), \"passthrough\"],       # Standardisation on/off,\n",
        "        \"imputer__strategy\" : [\"mean\" , \"median\"],          # Mean vs Median imputer\n",
        "         \"classifier_LogReg__penalty\" : [\"l2\", None]        # Penalty used for regularisation\n",
        "    }\n",
        "]\n",
        "\n",
        "rskf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
        "\n",
        "cv = GridSearchCV(pipeline_LogReg, params, cv=rskf, scoring=\"accuracy\", n_jobs=-1)\n",
        "\n",
        "cv.fit(X_train, y_train)\n",
        "print(f'Best accuracy score: {cv.best_score_:.3f}\\n')\n",
        "print(f'Best parameter set: {cv.best_params_}\\n')\n",
        "print(\"Train Scores:\")\n",
        "print(f'{classification_report(y_train, cv.predict(X_train))}')\n",
        "\n",
        "preds = cv.predict(X_holdout)\n",
        "print(\"Test Scores:\")\n",
        "print(f'{classification_report(y_holdout, preds)}\\n')\n",
        "print(f'Test accuracy score: {accuracy_score(y_holdout, preds):.3f}')\n",
        "\n",
        "classifier_list.append(\"Logistic Regression\")\n",
        "accuracy_list.append(accuracy_score(y_holdout, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xOld_I8U1qy"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(preds, y_holdout)\n",
        "\n",
        "print(\"Confusion Matrix for the Logistic Regression:\")\n",
        "print(cm)\n",
        "# Most errors are from the classes 2 and 3."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline of Decisions Trees :\n",
        "\n",
        "pipeline_DecTree = Pipeline(\n",
        "    [\n",
        "        ( \"imputer\", SimpleImputer() ),                     # Imputer\n",
        "        ( \"scaler\", StandardScaler() ),                     # Standardisation\n",
        "        ( \"classifier_DecisionTree\", DecisionTreeClassifier() )       # Classifier Logitic Regression\n",
        "\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = [\n",
        "    {\n",
        "        \"scaler\" : [StandardScaler(), \"passthrough\"],       # Standardisation on/off,\n",
        "        \"imputer__strategy\" : [\"mean\" , \"median\"],          # Mean vs Median imputer\n",
        "        \"classifier_DecisionTree__criterion\" : [\"gini\", \"entropy\", \"log_loss\"],     # Splitting criterion\n",
        "        \"classifier_DecisionTree__splitter\" : [\"best\", \"random\"],                   # Strategy for splitting\n",
        "        \"classifier_DecisionTree__max_depth\" : [i for i in range(10,31,5)]          # Maximum depth\n",
        "    }\n",
        "]\n",
        "\n",
        "rskf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
        "\n",
        "cv = GridSearchCV(pipeline_DecTree, params, cv=rskf, scoring=\"accuracy\", n_jobs=-1)\n",
        "\n",
        "cv.fit(X_train, y_train)\n",
        "print(f'Best accuracy score: {cv.best_score_:.3f}\\n')\n",
        "print(f'Best parameter set: {cv.best_params_}\\n')\n",
        "print(\"Train Scores:\")\n",
        "print(f'{classification_report(y_train, cv.predict(X_train))}')\n",
        "\n",
        "preds = cv.predict(X_holdout)\n",
        "print(\"Test Scores:\")\n",
        "print(f'{classification_report(y_holdout, preds)}\\n')\n",
        "print(f'Test accuracy score: {accuracy_score(y_holdout, preds):.3f}')\n",
        "\n",
        "classifier_list.append(\"Decision Trees\")\n",
        "accuracy_list.append(accuracy_score(y_holdout, preds))"
      ],
      "metadata": {
        "id": "Ju-K1LfJUngN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(preds, y_holdout)\n",
        "\n",
        "print(\"Confusion Matrix for the Decision Trees:\")\n",
        "print(cm)\n",
        "# Once again, most errors are from the classes 2 and 3."
      ],
      "metadata": {
        "id": "Opd7xy40UnMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJT3i63HlE5T"
      },
      "outputs": [],
      "source": [
        "# Pipeline of Neural Networks :\n",
        "\n",
        "pipeline_NN = Pipeline(\n",
        "    [\n",
        "        ( \"imputer\", SimpleImputer() ),                     # Imputer\n",
        "        ( \"scaler\", StandardScaler() ),                     # Standardisation\n",
        "        ( \"classifier_NN\", MLPClassifier() )                # Classifier Neural Network\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = [\n",
        "    {\n",
        "        \"scaler\" : [StandardScaler(), \"passthrough\"],       # Standardisation on/off,\n",
        "        \"imputer__strategy\" : [\"mean\" , \"median\"],          # Mean vs Median imputer\n",
        "        \"classifier_NN__activation\" : [\"identity\", \"logistic\", \"tanh\", \"relu\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "rskf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "cv = GridSearchCV(pipeline_NN, params, cv=rskf, scoring=\"accuracy\", n_jobs=-1)\n",
        "\n",
        "cv.fit(X_train, y_train)\n",
        "print(f'Best accuracy score: {cv.best_score_:.3f}\\n')\n",
        "print(f'Best parameter set: {cv.best_params_}\\n')\n",
        "print(\"Train Scores:\")\n",
        "print(f'{classification_report(y_train, cv.predict(X_train))}')\n",
        "\n",
        "preds = cv.predict(X_holdout)\n",
        "print(\"Test Scores:\")\n",
        "print(f'{classification_report(y_holdout, preds)}\\n')\n",
        "print(f'Test accuracy score: {accuracy_score(y_holdout, preds):.3f}')\n",
        "\n",
        "classifier_list.append(\"Neural Network\")\n",
        "accuracy_list.append(accuracy_score(y_holdout, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2ToSkI-Wjk1"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(preds, y_holdout)\n",
        "\n",
        "print(\"Confusion Matrix of the neural network:\")\n",
        "print(cm)\n",
        "# The only errors are from the classes 2 and 3."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary:\n",
        "print(\"Performance when we consider the 4 classes of the dataset\")\n",
        "print(\"Here are the differents accuracy score obtained after getting the best parameters for each classifier:\")\n",
        "for i in range(4):\n",
        "    print(\"Classifier: \" + classifier_list[i] + \", accuracy = \", accuracy_list[i])"
      ],
      "metadata": {
        "id": "HQytvHkJVdya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsuxnfI5wFRt"
      },
      "source": [
        "# IV. Reduction of dimension :"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We split the dataset into a training set and a test set:\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(\n",
        "    X_data, y_data, stratify = y_data, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "ftXxxdKfrgG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eB88Br7CwNm1"
      },
      "outputs": [],
      "source": [
        "# Applying PCA :\n",
        "\n",
        "pipeline_LogReg = Pipeline(\n",
        "    [\n",
        "        ( \"imputer\", SimpleImputer() ),                     # Imputer\n",
        "        ( \"scaler\", StandardScaler() ),                     # Standardisation\n",
        "        ( \"pca\", PCA() ),                                   # We apply PCA\n",
        "        ( \"classifier_SVC\", LogisticRegression() )          # Classifier Logistic Regression\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = [\n",
        "    {\n",
        "        \"scaler\" : [StandardScaler()],           # Standardisation\n",
        "        \"imputer__strategy\" : [\"mean\"],          # Imputer = mean\n",
        "        \"pca__n_components\" : [2*i for i in range(1,28)]     # Number of features to keep\n",
        "    }\n",
        "]\n",
        "\n",
        "rskf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
        "\n",
        "cv = GridSearchCV(pipeline_LogReg, params, cv=rskf, scoring=\"accuracy\", n_jobs=-1)\n",
        "\n",
        "cv.fit(X_train, y_train)\n",
        "print(f'Best accuracy score: {cv.best_score_:.3f}\\n')\n",
        "print(f'Best parameter set: {cv.best_params_}\\n')\n",
        "print(\"Train Scores:\")\n",
        "print(f'{classification_report(y_train, cv.predict(X_train))}')\n",
        "\n",
        "preds = cv.predict(X_holdout)\n",
        "print(\"Test Scores:\")\n",
        "print(f'{classification_report(y_holdout, preds)}\\n')\n",
        "print(f'Test accuracy score: {accuracy_score(y_holdout, preds):.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31eb-Cx7wN5r"
      },
      "outputs": [],
      "source": [
        "# Let's plot our accuracy:\n",
        "accuracy = np.zeros(27)\n",
        "for i in range(10):\n",
        "    for j in range(27):\n",
        "        accuracy[j] += cv.cv_results_[\"split\"+str(i)+\"_test_score\"][j]\n",
        "accuracy = accuracy/10\n",
        "\n",
        "number_of_features = [2*i for i in range(1,28)]\n",
        "\n",
        "plt.plot(number_of_features, accuracy)\n",
        "plt.xlabel(\"Number of features\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.scatter(2*(np.argmax(accuracy)+1), np.max(accuracy), color=\"red\", marker=\"x\")\n",
        "plt.grid(visible=True)\n",
        "plt.show()\n",
        "print(\"Maximum accuracy is reached with\", 2*(np.argmax(accuracy)+1), \"features.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiJhilHPeFSY"
      },
      "outputs": [],
      "source": [
        "# We test the Neural Network to see if we can reduce the number of features:\n",
        "\n",
        "pipeline_NN = Pipeline(\n",
        "    [\n",
        "        ( \"imputer\", SimpleImputer() ),               # Imputer\n",
        "        ( \"scaler\", StandardScaler() ),               # Standardisation\n",
        "        ( \"pca\", PCA() ),                             # We apply PCA\n",
        "        ( \"classifier_NN\", MLPClassifier() )          # Classifier Logistic Regression\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = [\n",
        "    {\n",
        "        \"scaler\" : [StandardScaler()],           # Standardisation\n",
        "        \"imputer__strategy\" : [\"mean\"],          # Imputer = mean\n",
        "        \"pca__n_components\" : [2*i for i in range(1,28)]     # Number of features to keep\n",
        "    }\n",
        "]\n",
        "\n",
        "rskf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "cv = GridSearchCV(pipeline_NN, params, cv=rskf, scoring=\"accuracy\", n_jobs=-1)\n",
        "\n",
        "cv.fit(X_train, y_train)\n",
        "print(f'Best accuracy score: {cv.best_score_:.3f}\\n')\n",
        "print(f'Best parameter set: {cv.best_params_}\\n')\n",
        "print(\"Train Scores:\")\n",
        "print(f'{classification_report(y_train, cv.predict(X_train))}')\n",
        "\n",
        "preds = cv.predict(X_holdout)\n",
        "print(\"Test Scores:\")\n",
        "print(f'{classification_report(y_holdout, preds)}\\n')\n",
        "print(f'Test accuracy score: {accuracy_score(y_holdout, preds):.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPB83v44eFuD"
      },
      "outputs": [],
      "source": [
        "# Let's plot our accuracy:\n",
        "accuracy = np.zeros(27)\n",
        "for i in range(5):\n",
        "    for j in range(27):\n",
        "        accuracy[j] += cv.cv_results_[\"split\"+str(i)+\"_test_score\"][j]\n",
        "accuracy = accuracy/5\n",
        "\n",
        "number_of_features = [2*i for i in range(1,28)]\n",
        "\n",
        "plt.plot(number_of_features, accuracy)\n",
        "plt.xlabel(\"Number of features\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.scatter(2*(np.argmax(accuracy)+1), np.max(accuracy), color=\"red\", marker=\"x\")\n",
        "plt.grid(visible=True)\n",
        "plt.show()\n",
        "print(\"La précision maximale est atteinte avec\", 2*(np.argmax(accuracy)+1), \"features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDsfGYAPaEVx"
      },
      "outputs": [],
      "source": [
        "# We now create a pipeline of Neural Networks with this number of features :\n",
        "\n",
        "pipeline_NN = Pipeline(\n",
        "    [\n",
        "        ( \"imputer\", SimpleImputer() ),                     # Imputer\n",
        "        ( \"scaler\", StandardScaler() ),                     # Standardisation\n",
        "        ( \"pca\", PCA() ),                                   # PCA\n",
        "        ( \"classifier_NN\", MLPClassifier() )                # Classifier Neural Network\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = [\n",
        "    {\n",
        "        \"scaler\" : [StandardScaler(), \"passthrough\"],        # Standardisation on/off,\n",
        "        \"imputer__strategy\" : [\"mean\", \"median\"],            # Mean vs Median imputer\n",
        "        \"classifier_NN__activation\" : [\"relu\"],\n",
        "        \"pca__n_components\" : [2*(np.argmax(accuracy)+1)],   # We use the number of components that maximises the accuracy according to the previous pipeline.\n",
        "        \"classifier_NN__hidden_layer_sizes\" : [(50,25),(75,),(100,50,25),(200,100,50,25)]\n",
        "    }\n",
        "]\n",
        "\n",
        "rskf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "cv = GridSearchCV(pipeline_NN, params, cv=rskf, scoring=\"accuracy\", n_jobs=-1)\n",
        "\n",
        "cv.fit(X_train, y_train)\n",
        "print(f'Best accuracy score: {cv.best_score_:.3f}\\n')\n",
        "print(f'Best parameter set: {cv.best_params_}\\n')\n",
        "print(\"Train Scores:\")\n",
        "print(f'{classification_report(y_train, cv.predict(X_train))}')\n",
        "\n",
        "preds = cv.predict(X_holdout)\n",
        "print(\"Test Scores:\")\n",
        "print(f'{classification_report(y_holdout, preds)}\\n')\n",
        "print(f'Test accuracy score: {accuracy_score(y_holdout, preds):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wn9VrSUOXZ8g"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(preds, y_holdout)\n",
        "\n",
        "print(\"Confusion Matrix of the neural network classifier with 38 features.\")\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-IwppYgYVSK"
      },
      "outputs": [],
      "source": [
        "# We do a quick analysis on the variance of the dataset:\n",
        "\n",
        "# We preprocess the data using the best scaler and imputer according to the Pipeline:\n",
        "imp = SimpleImputer(strategy = \"mean\")\n",
        "scaler = StandardScaler()\n",
        "X_PCA = scaler.fit_transform(X_data)\n",
        "X_PCA = imp.fit_transform(X_data)\n",
        "\n",
        "# We apply the PCA:\n",
        "pca = PCA()\n",
        "pca.fit(X_PCA)\n",
        "\n",
        "# The cumulative variance ratio will allow us to see how many features are needed to keep 99% of the information:\n",
        "cumulative_variance_ratio = pca.explained_variance_ratio_.cumsum()\n",
        "nb_comp = (cumulative_variance_ratio < 0.99).sum() + 1\n",
        "print(\"We keep 99% of the information with\", nb_comp , \"features.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRlu8_kNZDRE"
      },
      "outputs": [],
      "source": [
        "# Test Neural Network with a PCA that keeps 99% of the information:\n",
        "\n",
        "pipeline_NN = Pipeline(\n",
        "    [\n",
        "        ( \"imputer\", SimpleImputer() ),                     # Imputer\n",
        "        ( \"scaler\", StandardScaler() ),                     # Standardisation\n",
        "        ( \"pca\", PCA(n_components=nb_comp) ),               # PCA\n",
        "        ( \"classifier_NN\", MLPClassifier() )                # Classifier Neural Network\n",
        "    ]\n",
        ")\n",
        "\n",
        "params = [\n",
        "    {\n",
        "        \"scaler\" : [StandardScaler(),\"passthrough\"],        # Standardisation on/off,\n",
        "        \"imputer__strategy\" : [\"mean\" , \"median\"],          # Mean vs Median imputer\n",
        "        \"classifier_NN__activation\" : [\"relu\"],\n",
        "        \"classifier_NN__hidden_layer_sizes\" : [(50,25),(75,),(100,50,25),(200,100,50,25)]\n",
        "    }\n",
        "]\n",
        "\n",
        "rskf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "cv = GridSearchCV(pipeline_NN, params, cv=rskf, scoring=\"accuracy\", n_jobs=-1)\n",
        "\n",
        "cv.fit(X_train, y_train)\n",
        "print(f'Best accuracy score: {cv.best_score_:.3f}\\n')\n",
        "print(f'Best parameter set: {cv.best_params_}\\n')\n",
        "print(\"Train Scores:\")\n",
        "print(f'{classification_report(y_train, cv.predict(X_train))}')\n",
        "\n",
        "preds = cv.predict(X_holdout)\n",
        "print(\"Test Scores:\")\n",
        "print(f'{classification_report(y_holdout, preds)}\\n')\n",
        "print(f'Test accuracy score: {accuracy_score(y_holdout, preds):.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5AeimTYcpdy"
      },
      "outputs": [],
      "source": [
        "# We can see that the accuracy is lower:\n",
        "cm = confusion_matrix(preds, y_holdout)\n",
        "\n",
        "print(\"Confusion Matrix of the neural network classifier with \" + str(nb_comp) + \" features.\")\n",
        "print(cm)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}